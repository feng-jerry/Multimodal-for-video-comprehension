{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de4466-fbca-4d7b-8d7c-e863a2549e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Processor, HubertModel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a0609f-be71-4ff5-b522-b1ac08bcc585",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf3387b-ea8b-465d-a205-c282c09f8cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_feature = torch.load(\"/project/msoleyma_1026/Aff-Wild2/audio_feature/103-30-384x480.pt\", map_location=torch.device(device)).to(device)\n",
    "# video_feature = torch.tensor(np.load(\"/project/msoleyma_1026/Aff-Wild2/video_feature/103-30-384x480.npy\")).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dae0ea-9c3a-4da3-a6cf-1e2286ebdb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_feature_generation(video_feature, audio_feature, seq_len, pooling = \"mean\"):\n",
    "    #video_feature : (771, 1, 197, 768)\n",
    "    #audio_feature : [1, 773, 1024]\n",
    "    video_feature = torch.tensor(video_feature, dtype=torch.float32).to(device)\n",
    "    audio_feature = torch.tensor(audio_feature, dtype=torch.float32).to(device)\n",
    "\n",
    "    video_feature = video_feature.permute(1,0,2,3)\n",
    "    \n",
    "    if pooling == \"mean\":\n",
    "        video_feature = torch.mean(video_feature, dim = 2, keepdim=False)\n",
    "    elif pooling == \"max\":\n",
    "        video_feature = torch.max(video_feature, dim = 2, keepdim=False)[0]\n",
    "\n",
    "    max_seq = min(video_feature.shape[1], audio_feature.shape[1])\n",
    "    video_feature = video_feature[:, :max_seq, :]\n",
    "    audio_feature = audio_feature[:, :max_seq, :]\n",
    "    combined_feature = torch.cat([video_feature, audio_feature], dim = -1)\n",
    "    #[1, max_seq, 1024 + 768]\n",
    "    \n",
    "    if max_seq < seq_len:\n",
    "        # Pad both features to seq_len along the sequence dimension\n",
    "        combined_sequences = F.pad(combined_feature, (0, 0, 0, seq_len - max_seq))\n",
    "    else:\n",
    "        num_complete_seqs = max_seq // seq_len\n",
    "        combined_sequences = combined_feature[:,:num_complete_seqs*seq_len, :].view(-1, seq_len, combined_feature.shape[-1])\n",
    "    #[-1, seq_len, combined_feature_size]\n",
    "    return combined_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836a93d-b837-442e-92ac-f076d35cab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence = seq_feature_generation(video_feature, audio_feature, seq_len = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a603ec-8b67-424a-a0a2-a1d3ec207c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c31ab-cc7b-47bf-b193-242ce184ac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTHuBERTTransformer(nn.Module):\n",
    "    def __init__(self, vit_base_model,\n",
    "                 hubert_base_model,\n",
    "                 num_classes,\n",
    "                 nhead,\n",
    "                 num_layers,\n",
    "                small_dataset = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vit = timm.create_model(vit_base_model, pretrained=True)\n",
    "\n",
    "        #self.processor = Wav2Vec2Processor.from_pretrained(hubert_base_model)\n",
    "        self.hubert = HubertModel.from_pretrained(hubert_base_model)\n",
    "\n",
    "        if small_dataset:\n",
    "            for param in self.vit.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "            for param in self.hubert.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model = self.vit.num_features + self.hubert.config.hidden_size,\n",
    "                                                  nhead = nhead,\n",
    "                                                  dim_feedforward = (self.vit.num_features + self.hubert.config.hidden_size)//2,\n",
    "                                                  batch_first = True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers = num_layers)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(self.vit.num_features + self.hubert.config.hidden_size, num_classes)\n",
    "    def forward(self, video_feature_raw, audio_feature_raw):\n",
    "\n",
    "        vit_feature = self.vit.forward_features(video_feature_raw)\n",
    "        audio_feature = self.hubert(audio_feature_raw).last_hidden_state\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = torch.cat((vit_feature, audio_feature), dim=1)\n",
    "\n",
    "        transformer_output = self.transformer_encoder(combined_features)\n",
    "\n",
    "        logits = self.classifier(transformer_output.squeeze(1))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab43e8b-6d44-4d2d-ac47-12ac8142d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTHuBERTTransformer_prepossed(nn.Module):\n",
    "    def __init__(self, vit_base_model,\n",
    "                 hubert_base_model,\n",
    "                 num_classes,\n",
    "                 nhead,\n",
    "                 num_layers,\n",
    "                small_dataset = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vit = timm.create_model(vit_base_model, pretrained=True)\n",
    "\n",
    "        #self.processor = Wav2Vec2Processor.from_pretrained(hubert_base_model)\n",
    "        self.hubert = HubertModel.from_pretrained(hubert_base_model)\n",
    "\n",
    "        if small_dataset:\n",
    "            for param in self.vit.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "            for param in self.hubert.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model = self.vit.num_features + self.hubert.config.hidden_size,\n",
    "                                                  nhead = nhead,\n",
    "                                                  dim_feedforward = (self.vit.num_features + self.hubert.config.hidden_size)//2,\n",
    "                                                  batch_first = True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers = num_layers)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(self.vit.num_features + self.hubert.config.hidden_size, num_classes)\n",
    "    def forward(self, combined_feature):\n",
    "\n",
    "        transformer_output = self.transformer_encoder(combined_feature)\n",
    "\n",
    "        logits = self.classifier(transformer_output.squeeze(1))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f35470-a21f-4420-a0f2-e633d04cda1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTHuBERTTransformer_prepossed(\n",
    "    vit_base_model = 'vit_base_patch16_224',\n",
    "    hubert_base_model = \"facebook/hubert-large-ls960-ft\",\n",
    "    num_classes = 12,\n",
    "    nhead = 8,\n",
    "    num_layers = 6,\n",
    "    small_dataset = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afadb220-4a2d-4229-b466-15848b37abdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91296559-d14a-4620-8158-4689e3b0c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(sequence).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa5a74b-7bad-43da-8ae0-b5bea2b9f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    loss_cumulative = 0.\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for j, d in enumerate(dataloader):\n",
    "            d.to(device)\n",
    "            output = model(d)\n",
    "            #print(len(output))\n",
    "            #print(len(d.target))\n",
    "            loss = loss_fn(output, d.target).cpu()\n",
    "            loss_cumulative = loss_cumulative + loss.detach().item()\n",
    "    return loss_cumulative / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342cf6ca-4098-449d-9480-92cba6e8c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglinspace(rate, step, end=None):\n",
    "    t = 0\n",
    "    while end is None or t <= end:\n",
    "        yield t\n",
    "        t = int(t + 1 + step * (1 - math.exp(-t * rate / step)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a223d80-73fa-49cd-b61d-a165c9613063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader_train, dataloader_valid, loss_fn,\n",
    "             max_iter=101, scheduler=None, device=\"cpu\"):\n",
    "    model.to(device = device, dtype=torch.float32)\n",
    "    print(device)\n",
    "    checkpoint_generator = loglinspace(0.3, 5)\n",
    "    checkpoint = next(checkpoint_generator)\n",
    "    start_time = time.time()\n",
    "    run_name = \"vithubertformer\"\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(run_name + '.torch')['state'])\n",
    "    except:\n",
    "        results = {}\n",
    "        history = []\n",
    "        s0 = 0\n",
    "    else:\n",
    "        results = torch.load(run_name + '.torch')\n",
    "        history = results['history']\n",
    "        s0 = history[-1]['step'] + 1\n",
    "\n",
    "    for step in range(max_iter):\n",
    "        model.train()\n",
    "        loss_cumulative = 0.\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        for inputs, labels in tqdm(dataloader_train, desc=\"Training\"):\n",
    "            inputs, labels = inputs.squeeze().to(device), labels.squeeze().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_cumulative += loss.item()\n",
    "            \n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        train_f1_score = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "        model.eval()\n",
    "        val_labels = []\n",
    "        val_preds = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader_valid:\n",
    "                inputs, labels = inputs.squeeze().to(device), labels.squeeze().to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_loss = loss_fn(outputs, labels)\n",
    "\n",
    "                # Gather validation data for F1 score computation\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "        # Calculate F1 score for the validation set\n",
    "        val_f1_score = f1_score(val_labels, val_preds, average='macro')\n",
    "        print(f'{step}th epoch train_f1_score:', np.round(train_f1_score,4), 'val_f1_score:', np.round(val_f1_score,4))\n",
    "        \n",
    "        if step == checkpoint:\n",
    "            checkpoint = next(checkpoint_generator)\n",
    "            assert checkpoint > step\n",
    "\n",
    "            valid_avg_loss = evaluate(model, dataloader_valid, loss_fn, device)\n",
    "            train_avg_loss = evaluate(model, dataloader_train, loss_fn, device)\n",
    "\n",
    "            history.append({\n",
    "                'step': s0 + step,\n",
    "                'wall': time.time()-start_time,\n",
    "                'batch': {\n",
    "                    'loss': loss.item(),\n",
    "                },\n",
    "                'valid': {\n",
    "                    'loss': valid_avg_loss,\n",
    "                },\n",
    "                'train': {\n",
    "                    'loss': train_avg_loss,\n",
    "                },\n",
    "            })\n",
    "\n",
    "            results = {\n",
    "                'history': history,\n",
    "                'state': model.state_dict()\n",
    "            }\n",
    "\n",
    "            print(f\"epoch {step + 1:4d}   \" +\n",
    "                  f\"abs = {train_avg_loss:8.4f}   \" +\n",
    "                  f\"valid loss mse= {valid_avg_loss[0]:8.4f}   \" +\n",
    "                  f\"wall = {time.strftime('%H:%M:%S', time.gmtime(wall))}\")\n",
    "\n",
    "            with open(run_name + '.torch', 'wb') as f:\n",
    "                torch.save(results, f)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb5a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioVideoDataset(Dataset):\n",
    "    def __init__(self, video_dir, audio_dir, label_dir):\n",
    "        self.video_dir = video_dir\n",
    "        self.audio_dir = audio_dir\n",
    "        self.label_dir = label_dir\n",
    "\n",
    "        # Collect all label files, and construct corresponding video and audio file paths\n",
    "        self.entries = []\n",
    "        for label_file in sorted(os.listdir(label_dir)):\n",
    "            if label_file.endswith('.txt'):\n",
    "                base_name = os.path.splitext(label_file)[0]\n",
    "                video_file = os.path.join(video_dir, f\"{base_name}.npy\")\n",
    "                audio_file = os.path.join(audio_dir, f\"{base_name}.pt\")\n",
    "                label_file = os.path.join(label_dir, label_file)\n",
    "                if os.path.exists(video_file) and os.path.exists(audio_file):\n",
    "                    self.entries.append((video_file, audio_file, label_file))\n",
    "                else:\n",
    "                    print(f\"Missing video or audio file for {label_file}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_file, audio_file, label_file = self.entries[idx]\n",
    "        video_feature = np.load(video_file)\n",
    "        audio_feature = torch.load(audio_file)\n",
    "        labels = np.loadtxt(label_file, skiprows=1, delimiter=',')\n",
    "        \n",
    "        seq_len = 10  # Define the desired sequence length\n",
    "        min_len = min(labels.shape[0], video_feature.shape[0], audio_feature.shape[1])\n",
    "        #video_feature : (771, 1, 197, 768)\n",
    "        #audio_feature : [1, 773, 1024]  \n",
    "        #label : [771,12]\n",
    "        labels = labels[:min_len, :]\n",
    "        video_feature = video_feature[:min_len, :, :, :]\n",
    "        audio_feature = audio_feature[:, :min_len, :]\n",
    "        \n",
    "        combined_features = seq_feature_generation(video_feature, audio_feature, seq_len)  # Adjust device as needed\n",
    "\n",
    "        label_sequences = labels[:combined_features.shape[0] * seq_len].reshape(-1, seq_len, 12)\n",
    "\n",
    "        return combined_features, label_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fa2976-734a-44cc-9d20-b9a120849251",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_label_dir = '/project/msoleyma_1026/Aff-Wild2/labels/AU_Detection_Challenge/Train_Set'\n",
    "audio_feature_dir = '/project/msoleyma_1026/Aff-Wild2/features4'\n",
    "video_feature_dir = '/project/msoleyma_1026/Aff-Wild2/video_feature'\n",
    "dataset_train = AudioVideoDataset(video_feature_dir, audio_feature_dir, train_label_dir)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a76d291-87f9-417b-9423-62461fda8172",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a0ba89-bdbf-4a1d-b5b1-a7afddb7fa17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_label_dir = '/project/msoleyma_1026/Aff-Wild2/labels/AU_Detection_Challenge/Validation_Set'\n",
    "dataset_val = AudioVideoDataset(video_feature_dir, audio_feature_dir, val_label_dir)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce669f42-da38-490e-95ec-607d09fbab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b822b7d-3f11-472e-99be-9f74f504dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract one batch from the dataloader\n",
    "# for features, labels in dataloader_train:\n",
    "#     print(\"Features shape:\", features.shape)\n",
    "#     print(\"Labels shape:\", labels.shape)\n",
    "#     # print(\"Features example (one batch):\", features)\n",
    "#     # print(\"Labels example (one batch):\", labels)\n",
    "#     break  # Break after the first batch to only print one batch of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ce6997-4878-484e-ad26-05f58e6058c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.96)\n",
    "train(model, opt, dataloader_train, dataloader_val, loss_function,\n",
    "             max_iter=10, scheduler=scheduler, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3fb884-f69b-4b5a-b6bf-a855f80c2d28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
