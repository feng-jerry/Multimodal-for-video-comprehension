{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab8cab02-175b-4dae-bb2b-f7fe2d84f793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Processor, HubertModel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_video\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import torchaudio\n",
    "from moviepy.editor import VideoFileClip, vfx\n",
    "import logging\n",
    "logging.getLogger('moviepy').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12215279-e415-4777-8039-1d062fd2ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torchaudio.set_audio_backend(\"sox_io\")  # 或者 \"sox_io\"\n",
    "defalut_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c0d98d5-43a8-417a-8739-d2714af7c88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "505d69a8-3a78-4ba5-9329-97764de15909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transform():\n",
    "    #transform image to image feature\n",
    "    return transforms.Compose([\n",
    "        transforms.ToPILImage(),  # 将 numpy 数组或 tensor 转换为 PIL 图像\n",
    "        transforms.Resize((224, 224)),  # 调整图像大小\n",
    "        transforms.ToTensor(),  # 将 PIL 图像转换为 tensor，并归一化至 [0,1]\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 标准化\n",
    "    ])\n",
    "\n",
    "def audio_pre(audio, info):\n",
    "    \"\"\"\n",
    "    input:\n",
    "            audio preprocessed by Wav2Vec2Processor\n",
    "    return:\n",
    "            after channel and fps adjusted audio\n",
    "    \"\"\"\n",
    "    if audio.shape[0] > 1:  # More than one channel\n",
    "        audio = torch.mean(audio, dim=0, keepdim=True)\n",
    "\n",
    "    if info[\"audio_fps\"] != 16000:\n",
    "    # set sample rate to int(16000 * (0.02002 * info[\"video_fps\"])), but\n",
    "    # treat it as 16000, so that the audio speed is adjusted, in order to make\n",
    "    # the generated feature length the same as number of frames\n",
    "    # 0.02002 is a constant for HuBERT model\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=info[\"audio_fps\"],new_freq=int(16000 * (0.02002 * info[\"video_fps\"])))\n",
    "        resampled_audio = resampler(audio).squeeze()\n",
    "    \n",
    "    audio_feature = processor(resampled_audio, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "    audio_feature = audio_feature.to(device)\n",
    "\n",
    "    return audio_feature\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bb156de2-5888-4900-9f3b-64381985835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "video, audio, info = read_video(\"/project/msoleyma_1026/Aff-Wild2/video/326.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b5de9c8a-5fe0-4350-bc5f-160eb383892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_feature = audio_pre(audio, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5d584542-d627-4e78-a048-7fb7ee735239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1510687])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb135f6d-923a-41e5-a2e9-0a5ad6ccd978",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioVideoDataset(Dataset):\n",
    "    def __init__(self, video_dir, label_dir, device, frame_inteval=5, dtype=torch.float32):\n",
    "        self.video_dir = video_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.device = device\n",
    "        self.frame_inteval = frame_inteval\n",
    "        self.dtype = dtype\n",
    "        self.transform = self.create_transform()\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "        self.entries = self._collect_entries()\n",
    "\n",
    "    def _collect_entries(self):\n",
    "        possible_extensions = ['.mp4', '.avi']\n",
    "        entries = []\n",
    "        for label_file in sorted(os.listdir(self.label_dir)):\n",
    "            if label_file.endswith('.txt'):\n",
    "                base_name = os.path.splitext(label_file)[0]\n",
    "                video_file = self.find_video_file(self.video_dir, base_name, possible_extensions)\n",
    "                if video_file:\n",
    "                    entries.append((video_file, os.path.join(self.label_dir, label_file)))\n",
    "        return entries\n",
    "\n",
    "    def find_video_file(self, video_dir, base_name, possible_extensions):\n",
    "        for ext in possible_extensions:\n",
    "            video_path = os.path.join(video_dir, f\"{base_name}{ext}\")\n",
    "            if os.path.exists(video_path):\n",
    "                return video_path\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_file_path, label_file_path = self.entries[idx]\n",
    "        video, audio, info = read_video(video_file_path)\n",
    "\n",
    "        print(label_file_path)\n",
    "        \n",
    "        # Preprocess the audio first\n",
    "        audio_feature = self.audio_pre(audio, info)  # Assuming audio_pre is defined to handle this\n",
    "        \n",
    "        video_fps = info['video_fps']\n",
    "        frame_interval = max(1, int(video_fps / self.frame_inteval))\n",
    "        audio_duration = audio_feature.shape[1] // video.shape[0]\n",
    "        \n",
    "        sampled_video_frames = []\n",
    "        sampled_audio_frames = torch.empty((0,), dtype=self.dtype, device = self.device)  # Correct dtype\n",
    "        \n",
    "        labels = torch.tensor(np.loadtxt(label_file_path, skiprows=1, delimiter=','), dtype=self.dtype)\n",
    "        sampled_labels = []\n",
    "\n",
    "        for i in range(0, len(video), frame_interval):\n",
    "            frame = video[i].permute(2, 0, 1)\n",
    "            sampled_video_frames.append(self.transform(frame))\n",
    "            sampled_labels.append(labels[min(i, len(labels) - 1)])  # Safeguard index\n",
    "            \n",
    "            # Extract corresponding audio segment\n",
    "            start = i * audio_duration\n",
    "            end = (i + 1) * audio_duration\n",
    "            sampled_audio_segment = audio_feature[:, start:end]\n",
    "            sampled_audio_frames = torch.cat((sampled_audio_frames, sampled_audio_segment), dim=1)\n",
    "        \n",
    "        video_feature = torch.stack(sampled_video_frames)\n",
    "        labels_feature = torch.stack(sampled_labels)\n",
    "\n",
    "         #video_feature 此时 torch.Size([num_frames, 3, 224, 224])， audio_feature还为raw， 经过hubert后，转为 torch.Size([1, num_frames, 1024])， 需要在后续模型中进行对齐\n",
    "\n",
    "        return video_feature, sampled_audio_frames, labels_feature\n",
    "        \n",
    "    def create_transform(self):\n",
    "        #transform image to image feature\n",
    "        return transforms.Compose([\n",
    "            transforms.ToPILImage(),  # 将 numpy 数组或 tensor 转换为 PIL 图像\n",
    "            transforms.Resize((224, 224)),  # 调整图像大小\n",
    "            transforms.ToTensor(),  # 将 PIL 图像转换为 tensor，并归一化至 [0,1]\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 标准化\n",
    "        ])\n",
    "\n",
    "    def audio_pre(self,audio, info):\n",
    "        \"\"\"\n",
    "        input:\n",
    "                audio preprocessed by Wav2Vec2Processor\n",
    "        return:\n",
    "                after channel and fps adjusted audio\n",
    "        \"\"\"\n",
    "        if audio.shape[0] > 1:  # More than one channel\n",
    "            audio = torch.mean(audio, dim=0, keepdim=True)\n",
    "\n",
    "        if info[\"audio_fps\"] != 16000:\n",
    "        # set sample rate to int(16000 * (0.02002 * info[\"video_fps\"])), but\n",
    "        # treat it as 16000, so that the audio speed is adjusted, in order to make\n",
    "        # the generated feature length the same as number of frames\n",
    "        # 0.02002 is a constant for HuBERT model\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=info[\"audio_fps\"],new_freq=int(16000 * (0.02002 * info[\"video_fps\"])))\n",
    "            resampled_audio = resampler(audio).squeeze()\n",
    "        \n",
    "        audio_feature = self.processor(resampled_audio, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "        audio_feature = audio_feature.to(device, dtype = self.dtype)\n",
    "\n",
    "        return audio_feature\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "163c1c0c-7941-44d4-9876-a9a26a9732a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "#train_label_dir = \"/project/msoleyma_1026/Aff-Wild2/labels/EXPR_Classification_Challenge/Train_Set\"\n",
    "#val_label_dir = \"/project/msoleyma_1026/Aff-Wild2/labels/EXPR_Classification_Challenge/Validation_Set\"\n",
    "video_dir = \"/project/msoleyma_1026/Aff-Wild2/video\"\n",
    "\n",
    "train_label_dir = \"/project/msoleyma_1026/Aff-Wild2/test/test_3_expr/Train\"\n",
    "val_label_dir = \"/project/msoleyma_1026/Aff-Wild2/test/test_3_expr/Val\"\n",
    "\n",
    "train_dataset = AudioVideoDataset(video_dir, train_label_dir, device)\n",
    "val_dataset = AudioVideoDataset(video_dir, val_label_dir, device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8547942-a269-4a37-a7f4-6464cafcc5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/msoleyma_1026/Aff-Wild2/test/test_3_expr/Train/325.txt\n"
     ]
    }
   ],
   "source": [
    "test = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e365934b-64c0-4095-ab4d-46c032e1ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTHuBERTTransformer(nn.Module):\n",
    "    def __init__(self, vit_base_model,\n",
    "                 hubert_base_model,\n",
    "                 num_classes,\n",
    "                 nhead,\n",
    "                 num_layers,\n",
    "                 seq_len,\n",
    "                small_dataset = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.num_classes = num_classes\n",
    "        self.vit = timm.create_model(vit_base_model, pretrained=True)\n",
    "\n",
    "        #self.processor = Wav2Vec2Processor.from_pretrained(hubert_base_model)\n",
    "        self.hubert = HubertModel.from_pretrained(hubert_base_model)\n",
    "\n",
    "        if small_dataset:\n",
    "            for param in self.vit.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "            for param in self.hubert.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model = self.vit.num_features + self.hubert.config.hidden_size,\n",
    "                                                  nhead = nhead,\n",
    "                                                  dim_feedforward = (self.vit.num_features + self.hubert.config.hidden_size)//2,\n",
    "                                                  batch_first = True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers = num_layers)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(self.vit.num_features + self.hubert.config.hidden_size, num_classes)\n",
    "    def forward(self, video_feature_raw, audio_feature_raw):\n",
    "\n",
    "        audio_feature = self.hubert(audio_feature_raw).last_hidden_state\n",
    "        vit_feature = self.vit.forward_features(video_feature_raw)\n",
    "\n",
    "        #print(audio_feature.shape)\n",
    "        #print(vit_feature.shape)\n",
    "\n",
    "        combined_features = self.seq_feature_generation(vit_feature, audio_feature, self.seq_len)\n",
    "        #[batch, seq_len, combined_feature_size]\n",
    "\n",
    "        #print(combined_features.shape)\n",
    "        transformer_output = self.transformer_encoder(combined_features)\n",
    "        \n",
    "        logits = self.classifier(transformer_output)\n",
    "        logits = logits.view(-1, self.num_classes)\n",
    "        return logits\n",
    "\n",
    "    def seq_feature_generation(self, video_feature, audio_feature, seq_len, pooling = \"mean\"):\n",
    "        #video_feature : (771, 1, 197, 768)\n",
    "        #audio_feature : [1, 773, 1024]\n",
    "        video_feature = torch.tensor(video_feature, dtype=torch.float32).unsqueeze(1)\n",
    "        audio_feature = torch.tensor(audio_feature, dtype=torch.float32)\n",
    "\n",
    "        if audio_feature.shape[1] < video_feature.shape[0]:\n",
    "            # 需要增加的长度\n",
    "            padding_length = video_feature.shape[0] - audio_feature.shape[1]\n",
    "            \n",
    "            # 取最后一个时间步的特征进行复制\n",
    "            last_frame_features = audio_feature[:, -1, :]\n",
    "            \n",
    "            # 重复最后一帧特征直到达到所需长度\n",
    "            padding = last_frame_features.repeat(1, padding_length, 1)\n",
    "            \n",
    "            # 将原始音频特征和填充特征拼接起来\n",
    "            padded_audio_feature = torch.cat([audio_feature, padding], dim=1)\n",
    "\n",
    "\n",
    "        #print(padded_audio_feature.shape)\n",
    "\n",
    "        video_feature = video_feature.permute(1,0,2,3)\n",
    "        \n",
    "        if pooling == \"mean\":\n",
    "            video_feature = torch.mean(video_feature, dim = 2, keepdim=False)\n",
    "        elif pooling == \"max\":\n",
    "            video_feature = torch.max(video_feature, dim = 2, keepdim=False)[0]\n",
    "    \n",
    "        max_seq = min(video_feature.shape[1], padded_audio_feature.shape[1])\n",
    "        video_feature = video_feature[:, :max_seq, :]\n",
    "        audio_feature = padded_audio_feature[:, :max_seq, :]\n",
    "\n",
    "        #print(video_feature.shape)\n",
    "        #print(audio_feature.shape)\n",
    "        \n",
    "        combined_feature = torch.cat([video_feature, audio_feature], dim = -1)\n",
    "        #[1, max_seq, 1024 + 768]\n",
    "\n",
    "        #print(combined_feature.shape)\n",
    "        \n",
    "        if max_seq < seq_len:\n",
    "            # Pad both features to seq_len along the sequence dimension\n",
    "            combined_sequences = F.pad(combined_feature, (0, 0, 0, seq_len - max_seq))\n",
    "        else:\n",
    "            num_complete_seqs = max_seq // seq_len\n",
    "            combined_sequences = combined_feature[:,:num_complete_seqs*seq_len, :].view(-1, seq_len, combined_feature.shape[-1])\n",
    "        #[batch, seq_len, combined_feature_size]\n",
    "\n",
    "        if combined_feature.shape[1]%self.seq_len !=0:\n",
    "            tmp_seq = (combined_feature.shape[1]//self.seq_len)*self.seq_len\n",
    "            combined_feature = combined_feature[:,:tmp_seq, :]\n",
    "            \n",
    "        return combined_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c66688a-a95b-4ca8-b5cc-5a5709236d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HubertModel were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ViTHuBERTTransformer(\n",
    "    vit_base_model = 'vit_base_patch16_224',\n",
    "    hubert_base_model = \"facebook/hubert-large-ls960-ft\",\n",
    "    num_classes = 8,\n",
    "    nhead = 8,\n",
    "    num_layers = 6,\n",
    "    seq_len = 10,\n",
    "    small_dataset = False\n",
    ")\n",
    "#model = model.half()  # Convert all model parameters and buffers to float16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5169a4e-759b-4db7-bba5-f5d3818636a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTHuBERTTransformer(\n",
       "  (vit): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       "  (hubert): HubertModel(\n",
       "    (feature_extractor): HubertFeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): HubertLayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x HubertLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x HubertLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): HubertFeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): HubertEncoderStableLayerNorm(\n",
       "      (pos_conv_embed): HubertPositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): HubertSamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=896, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=896, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=1792, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e76bb00-895a-4b53-adff-566f284858dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b01b48f7-f175-40ce-b4a3-e5427dc2ce14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f3b44fc-41c7-415a-afe1-8a6542940c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader_train, dataloader_valid, loss_fn,\n",
    "             seq_len = 10, batch_size = 3, max_iter=101, scheduler=None, device=\"cpu\"):\n",
    "    model.to(device = device, dtype=torch.float32)\n",
    "    print(device)\n",
    "    checkpoint_generator = loglinspace(0.3, 5)\n",
    "    checkpoint = next(checkpoint_generator)\n",
    "    start_time = time.time()\n",
    "    run_name = \"vithubertformer\"\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(run_name + '.torch')['state'])\n",
    "    except:\n",
    "        results = {}\n",
    "        history = []\n",
    "        s0 = 0\n",
    "    else:\n",
    "        results = torch.load(run_name + '.torch')\n",
    "        history = results['history']\n",
    "        s0 = history[-1]['step'] + 1\n",
    "\n",
    "    for step in range(max_iter):\n",
    "        model.train()\n",
    "        loss_cumulative = 0.\n",
    "\n",
    "        for j, d in tqdm(enumerate(dataloader_train), total=len(dataloader_train)):\n",
    "            video_feature, audio_feature, labels = d\n",
    "\n",
    "            video_feature = video_feature.squeeze(0).to(device)\n",
    "            audio_feature = audio_feature.squeeze(0).to(device)\n",
    "            labels = labels.squeeze(0).to(device)\n",
    "            \n",
    "            audio_duration = audio_feature.shape[1]//video_feature.shape[0]\n",
    "            for i in range(0, len(video_feature), seq_len*batch_size):\n",
    "                video_batch = video_feature[i:i + seq_len * batch_size]\n",
    "                audio_batch = audio_feature[:,i*audio_duration:(i + seq_len * batch_size)*audio_duration]\n",
    "                label_batch = labels[i:i + seq_len * batch_size]\n",
    "\n",
    "                #print(video_batch.shape)\n",
    "                #print(audio_batch.shape)\n",
    "                #print(label_batch.shape) 30\n",
    "            #print(model.dtype)\n",
    "                if label_batch.shape[0]%seq_len !=0:\n",
    "                    tmp =  (label_batch.shape[0]//seq_len)*seq_len\n",
    "                    label_batch = label_batch[:tmp]\n",
    "                \n",
    "                mask = (label_batch != -1)\n",
    "            \n",
    "                output = model(video_batch, audio_batch)\n",
    "                #print(output.shape)  (2, 10, 7)\n",
    "\n",
    "                filtered_output = output[mask]\n",
    "                filtered_label_batch = label_batch[mask]\n",
    "\n",
    "                #print(filtered_output.shape)\n",
    "                #print(filtered_label_batch.shape)\n",
    "                \n",
    "                loss = loss_fn(filtered_output, filtered_label_batch.long()).cpu()\n",
    "                loss_cumulative = loss_cumulative + loss.detach().item()\n",
    "    \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        end_time = time.time()\n",
    "        wall = end_time - start_time\n",
    "\n",
    "        if step == checkpoint:\n",
    "            checkpoint = next(checkpoint_generator)\n",
    "            assert checkpoint > step\n",
    "\n",
    "            valid_avg_loss = evaluate(model, dataloader_valid, loss_fn, device)\n",
    "            train_avg_loss = evaluate(model, dataloader_train, loss_fn, device)\n",
    "\n",
    "            history.append({\n",
    "                'step': s0 + step,\n",
    "                'wall': wall,\n",
    "                'batch': {\n",
    "                    'loss': loss.item(),\n",
    "                },\n",
    "                'valid': {\n",
    "                    'loss': valid_avg_loss,\n",
    "                },\n",
    "                'train': {\n",
    "                    'loss': train_avg_loss,\n",
    "                },\n",
    "            })\n",
    "\n",
    "            results = {\n",
    "                'history': history,\n",
    "                'state': model.state_dict()\n",
    "            }\n",
    "\n",
    "            print(f\"epoch {step + 1:4d}   \" +\n",
    "                  f\"abs = {train_avg_loss:8.4f}   \" +\n",
    "                  f\"valid loss mse= {valid_avg_loss[0]:8.4f}   \" +\n",
    "                  f\"wall = {time.strftime('%H:%M:%S', time.gmtime(wall))}\")\n",
    "\n",
    "            with open(run_name + '.torch', 'wb') as f:\n",
    "                torch.save(results, f)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    loss_cumulative = 0.\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for j, d in enumerate(dataloader):\n",
    "            video_feature, audio_feature, labels = d\n",
    "            \n",
    "            video_feature.to(device)\n",
    "            audio_feature.to(device)\n",
    "            labels.to(device)\n",
    "\n",
    "            output = model(video_feature, audio_feature)\n",
    "            #print(len(output))\n",
    "            #print(len(d.target))\n",
    "            loss = loss_fn(output, d.target).cpu()\n",
    "            loss_cumulative = loss_cumulative + loss.detach().item()\n",
    "    return loss_cumulative / len(dataloader)\n",
    "\n",
    "def loglinspace(rate, step, end=None):\n",
    "    t = 0\n",
    "    while end is None or t <= end:\n",
    "        yield t\n",
    "        t = int(t + 1 + step * (1 - math.exp(-t * rate / step)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "91e8543e-3a46-42d4-b9c7-a278c8bb43de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/msoleyma_1026/Aff-Wild2/test/test_3_expr/Train/325.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:12<01:15, 12.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/msoleyma_1026/Aff-Wild2/test/test_3_expr/Train/326.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:26<02:39, 26.52s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: 10 and `sequence_length`: 9`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, dataloader_train, dataloader_valid, loss_fn, seq_len, batch_size, max_iter, scheduler, device)\u001b[0m\n\u001b[1;32m     43\u001b[0m     label_batch \u001b[38;5;241m=\u001b[39m label_batch[:tmp]\n\u001b[1;32m     45\u001b[0m mask \u001b[38;5;241m=\u001b[39m (label_batch \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#print(output.shape)  (2, 10, 7)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m filtered_output \u001b[38;5;241m=\u001b[39m output[mask]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 36\u001b[0m, in \u001b[0;36mViTHuBERTTransformer.forward\u001b[0;34m(self, video_feature_raw, audio_feature_raw)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_feature_raw, audio_feature_raw):\n\u001b[0;32m---> 36\u001b[0m     audio_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhubert\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_feature_raw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     37\u001b[0m     vit_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvit\u001b[38;5;241m.\u001b[39mforward_features(video_feature_raw)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m#print(audio_feature.shape)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m#print(vit_feature.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/hubert/modeling_hubert.py:1086\u001b[0m, in \u001b[0;36mHubertModel.forward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_feature_vector_attention_mask(extract_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], attention_mask)\n\u001b[1;32m   1085\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_projection(extract_features)\n\u001b[0;32m-> 1086\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mask_hidden_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_time_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_time_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1089\u001b[0m     hidden_states,\n\u001b[1;32m   1090\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1094\u001b[0m )\n\u001b[1;32m   1096\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/hubert/modeling_hubert.py:1010\u001b[0m, in \u001b[0;36mHubertModel._mask_hidden_states\u001b[0;34m(self, hidden_states, mask_time_indices, attention_mask)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     hidden_states[mask_time_indices] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasked_spec_embed\u001b[38;5;241m.\u001b[39mto(hidden_states\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmask_time_prob \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m-> 1010\u001b[0m     mask_time_indices \u001b[38;5;241m=\u001b[39m \u001b[43m_compute_mask_indices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_time_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_time_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_time_min_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m     mask_time_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(mask_time_indices, device\u001b[38;5;241m=\u001b[39mhidden_states\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m   1018\u001b[0m     hidden_states[mask_time_indices] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasked_spec_embed\u001b[38;5;241m.\u001b[39mto(hidden_states\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/hubert/modeling_hubert.py:95\u001b[0m, in \u001b[0;36m_compute_mask_indices\u001b[0;34m(shape, mask_prob, mask_length, attention_mask, min_masks)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`mask_length` has to be bigger than 0.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask_length \u001b[38;5;241m>\u001b[39m sequence_length:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and `sequence_length`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     )\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# epsilon is used for probabilistic rounding\u001b[39;00m\n\u001b[1;32m    101\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mValueError\u001b[0m: `mask_length` has to be smaller than `sequence_length`, but got `mask_length`: 10 and `sequence_length`: 9`"
     ]
    }
   ],
   "source": [
    "train(model, opt, train_loader, val_loader, loss_function,max_iter=1, scheduler=scheduler, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fb689f-447a-4999-bbd9-07971d563c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c90ef5-5fd2-4910-b77e-6769d06f6f82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Multimodal",
   "language": "python",
   "name": "mm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
