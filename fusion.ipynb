{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52de4466-fbca-4d7b-8d7c-e863a2549e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yifan/anaconda3/envs/multimodal/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Processor, HubertModel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_video\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import torchaudio\n",
    "from moviepy.editor import VideoFileClip, vfx\n",
    "import logging\n",
    "logging.getLogger('moviepy').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a0609f-be71-4ff5-b522-b1ac08bcc585",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torchaudio.set_audio_backend(\"soundfile\")  # 或者 \"sox_io\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5f9301-e034-4772-b5ac-8d7729d62772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcfc997-018b-4b2e-9c68-55f93525cf87",
   "metadata": {},
   "source": [
    "## tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7dae0ea-9c3a-4da3-a6cf-1e2286ebdb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_feature_generation(video_feature, audio_feature, seq_len, pooling = \"mean\"):\n",
    "    #video_feature : (771, 1, 197, 768)\n",
    "    #audio_feature : [1, 773, 1024]\n",
    "    video_feature = torch.tensor(video_feature, dtype=torch.float32)\n",
    "    audio_feature = torch.tensor(audio_feature, dtype=torch.float32)\n",
    "\n",
    "    video_feature = video_feature.permute(1,0,2,3)\n",
    "    \n",
    "    if pooling == \"mean\":\n",
    "        video_feature = torch.mean(video_feature, dim = 2, keepdim=False)\n",
    "    elif pooling == \"max\":\n",
    "        video_feature = torch.max(video_feature, dim = 2, keepdim=False)[0]\n",
    "\n",
    "    max_seq = min(video_feature.shape[1], audio_feature.shape[1])\n",
    "    video_feature = video_feature[:, :max_seq, :]\n",
    "    audio_feature = audio_feature[:, :max_seq, :]\n",
    "    combined_feature = torch.cat([video_feature, audio_feature], dim = -1)\n",
    "    #[1, max_seq, 1024 + 768]\n",
    "    \n",
    "    if max_seq < seq_len:\n",
    "        # Pad both features to seq_len along the sequence dimension\n",
    "        combined_sequences = F.pad(combined_feature, (0, 0, 0, seq_len - max_seq))\n",
    "    else:\n",
    "        num_complete_seqs = max_seq // seq_len\n",
    "        combined_sequences = combined_feature[:,:num_complete_seqs*seq_len, :].view(-1, seq_len, combined_feature.shape[-1])\n",
    "    #[-1, seq_len, combined_feature_size]\n",
    "    return combined_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96813119-fea0-4c2d-ba31-a8f44a6e7a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglinspace(rate, step, end=None):\n",
    "    t = 0\n",
    "    while end is None or t <= end:\n",
    "        yield t\n",
    "        t = int(t + 1 + step * (1 - math.exp(-t * rate / step)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bab43e8b-6d44-4d2d-ac47-12ac8142d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTHuBERTTransformer_prepossed(nn.Module):\n",
    "    def __init__(self, vit_base_model,\n",
    "                 hubert_base_model,\n",
    "                 num_classes,\n",
    "                 nhead,\n",
    "                 num_layers,\n",
    "                small_dataset = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vit = timm.create_model(vit_base_model, pretrained=True)\n",
    "\n",
    "        #self.processor = Wav2Vec2Processor.from_pretrained(hubert_base_model)\n",
    "        self.hubert = HubertModel.from_pretrained(hubert_base_model)\n",
    "\n",
    "        if small_dataset:\n",
    "            for param in self.vit.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "            for param in self.hubert.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model = self.vit.num_features + self.hubert.config.hidden_size,\n",
    "                                                  nhead = nhead,\n",
    "                                                  dim_feedforward = (self.vit.num_features + self.hubert.config.hidden_size)//2,\n",
    "                                                  batch_first = True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers = num_layers)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(self.vit.num_features + self.hubert.config.hidden_size, num_classes)\n",
    "    def forward(self, combined_feature):\n",
    "\n",
    "        transformer_output = self.transformer_encoder(combined_feature)\n",
    "\n",
    "        logits = self.classifier(transformer_output.squeeze(1))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aa5a74b-7bad-43da-8ae0-b5bea2b9f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    loss_cumulative = 0.\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for j, d in enumerate(dataloader):\n",
    "            video_feature, audio_feature, labels = d\n",
    "            \n",
    "            video_feature.to(device)\n",
    "            audio_feature.to(device)\n",
    "            labels.to(device)\n",
    "\n",
    "            output = model(video_feature, audio_feature)\n",
    "            #print(len(output))\n",
    "            #print(len(d.target))\n",
    "            loss = loss_fn(output, d.target).cpu()\n",
    "            loss_cumulative = loss_cumulative + loss.detach().item()\n",
    "    return loss_cumulative / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a223d80-73fa-49cd-b61d-a165c9613063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader_train, dataloader_valid, loss_fn,\n",
    "             max_iter=101, scheduler=None, device=\"cpu\"):\n",
    "    model.to(device = device, dtype=torch.float32)\n",
    "    print(device)\n",
    "    checkpoint_generator = loglinspace(0.3, 5)\n",
    "    checkpoint = next(checkpoint_generator)\n",
    "    start_time = time.time()\n",
    "    run_name = \"vithubertformer\"\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(run_name + '.torch')['state'])\n",
    "    except:\n",
    "        results = {}\n",
    "        history = []\n",
    "        s0 = 0\n",
    "    else:\n",
    "        results = torch.load(run_name + '.torch')\n",
    "        history = results['history']\n",
    "        s0 = history[-1]['step'] + 1\n",
    "\n",
    "    for step in range(max_iter):\n",
    "        model.train()\n",
    "        loss_cumulative = 0.\n",
    "\n",
    "        for j, d in tqdm(enumerate(dataloader_train), total=len(dataloader_train)):\n",
    "            video_feature, audio_feature, labels = d\n",
    "            \n",
    "            video_feature = video_feature.squeeze(0).to(device)\n",
    "            audio_feature = audio_feature.squeeze(0).to(device)\n",
    "            labels = labels.squeeze(0).to(device)\n",
    "\n",
    "            print(video_feature.shape)\n",
    "            print(audio_feature.shape)\n",
    "            print(labels.shape)\n",
    "            \n",
    "            output = model(video_feature, audio_feature)\n",
    "            loss = loss_fn(output, labels).cpu()\n",
    "            loss_cumulative = loss_cumulative + loss.detach().item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        end_time = time.time()\n",
    "        wall = end_time - start_time\n",
    "\n",
    "        if step == checkpoint:\n",
    "            checkpoint = next(checkpoint_generator)\n",
    "            assert checkpoint > step\n",
    "\n",
    "            valid_avg_loss = evaluate(model, dataloader_valid, loss_fn, device)\n",
    "            train_avg_loss = evaluate(model, dataloader_train, loss_fn, device)\n",
    "\n",
    "            history.append({\n",
    "                'step': s0 + step,\n",
    "                'wall': wall,\n",
    "                'batch': {\n",
    "                    'loss': loss.item(),\n",
    "                },\n",
    "                'valid': {\n",
    "                    'loss': valid_avg_loss,\n",
    "                },\n",
    "                'train': {\n",
    "                    'loss': train_avg_loss,\n",
    "                },\n",
    "            })\n",
    "\n",
    "            results = {\n",
    "                'history': history,\n",
    "                'state': model.state_dict()\n",
    "            }\n",
    "\n",
    "            print(f\"epoch {step + 1:4d}   \" +\n",
    "                  f\"abs = {train_avg_loss:8.4f}   \" +\n",
    "                  f\"valid loss mse= {valid_avg_loss[0]:8.4f}   \" +\n",
    "                  f\"wall = {time.strftime('%H:%M:%S', time.gmtime(wall))}\")\n",
    "\n",
    "            with open(run_name + '.torch', 'wb') as f:\n",
    "                torch.save(results, f)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e107c-72e9-44a9-8025-f3003ebf96f2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f038268-12bd-4a3d-a55a-f8257d74a0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioVideoDataset(Dataset):\n",
    "    def __init__(self, video_dir, label_dir, device, seq_len):\n",
    "        self.video_dir = video_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.device = device\n",
    "        self.seq_len = seq_len\n",
    "        self.transform = self.create_transform()\n",
    "\n",
    "        possible_extensions = ['.mp4', '.avi']\n",
    "        # Collect all label files, and construct corresponding video and audio file paths\n",
    "        self.entries = []\n",
    "        for label_file in sorted(os.listdir(label_dir)):\n",
    "            if label_file.endswith('.txt'):\n",
    "                base_name = os.path.splitext(label_file)[0]\n",
    "                video_file = None\n",
    "                for ext in possible_extensions:\n",
    "                    video_path = os.path.join(video_dir, f\"{base_name}{ext}\")\n",
    "                    if os.path.exists(video_path):\n",
    "                        video_file = video_path\n",
    "                        break\n",
    "                label_file = os.path.join(label_dir, label_file)\n",
    "                \n",
    "                if os.path.exists(video_file):\n",
    "                    self.entries.append((video_file, label_file))\n",
    "                else:\n",
    "                    print(f\"Missing video or audio file for {label_file}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_file_path, label_file_path = self.entries[idx]\n",
    "        video, audio, info = read_video(video_file_path)\n",
    "        #print(video.shape) #[6893, 360, 640, 3] \n",
    "        #print(audio.shape) [1,18480840]\n",
    "        #print(info)\n",
    "        labels = torch.tensor(np.loadtxt(label_file_path, skiprows=1, delimiter=','))\n",
    "\n",
    "        video_feature = torch.stack([self.transform(frame.permute(2,0,1)) for frame in video])\n",
    "        #print(video_feature.shape)\n",
    "\n",
    "        audio_feature = self.audio_pre(video_file_path)\n",
    "        #audio_feature.shape\n",
    "        #video_feature 此时 torch.Size([6286, 3, 224, 224])， audio_feature还为raw， 经过hubert后，转为 torch.Size([1, 6291, 1024])， 需要在后续模型中进行对齐\n",
    "        return video_feature, audio_feature, labels\n",
    "\n",
    "    def create_transform(self):\n",
    "        #transform image to image feature\n",
    "        return transforms.Compose([\n",
    "            transforms.ToPILImage(),  # 将 numpy 数组或 tensor 转换为 PIL 图像\n",
    "            transforms.Resize((224, 224)),  # 调整图像大小\n",
    "            transforms.ToTensor(),  # 将 PIL 图像转换为 tensor，并归一化至 [0,1]\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 标准化\n",
    "        ])\n",
    "\n",
    "    def audio_pre(self,video_path):\n",
    "        #output audio_preprocessed feature before goes in hubert\n",
    "        processor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "        #CHUNK_SIZE = 60 * 16000\n",
    "        def load_audio(video_path):\n",
    "            video_clip = VideoFileClip(video_path)\n",
    "            audio_clip = video_clip.audio\n",
    "            # print(video_clip.reader.nframes)\n",
    "            new_audio = audio_clip.fx(vfx.speedx, 1 / (0.02002 * video_clip.fps))\n",
    "            new_audio.write_audiofile('output.wav')\n",
    "            return video_clip.reader.nframes\n",
    "\n",
    "        frames_number = load_audio(video_path) \n",
    "        #print(frames) #6286\n",
    "        audio_input, sample_rate = torchaudio.load(\"./output.wav\")\n",
    "        #print(audio_input.shape) torch.Size([2, 5549707])\n",
    "        #print(sample_rate) 44100\n",
    "        # Check if the audio is stereo and convert to mono if necessary\n",
    "        if audio_input.shape[0] > 1:  # More than one channel\n",
    "            audio_input = torch.mean(audio_input, dim=0, keepdim=True)\n",
    "    \n",
    "        # Resample the audio file if necessary\n",
    "        if sample_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "            audio_input = resampler(audio_input)\n",
    "\n",
    "        audio_input = audio_input.squeeze()\n",
    "        features = processor(audio_input, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "        return features\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c0d7cbd-4b29-466c-9498-bbb27dd6a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "train_label_dir = \"/home/yifan/Desktop/deep_learning/multimodal/Aff-Wild2/Aff-Wild2/labels/AU_Detection_Challenge/small_dataset_train\"\n",
    "val_label_dir = \"/home/yifan/Desktop/deep_learning/multimodal/Aff-Wild2/Aff-Wild2/labels/AU_Detection_Challenge/test\"\n",
    "video_dir = \"/home/yifan/Desktop/deep_learning/multimodal/Aff-Wild2/Aff-Wild2/video\"\n",
    "\n",
    "train_dataset = AudioVideoDataset(video_dir, train_label_dir, device, 10)\n",
    "val_dataset = AudioVideoDataset(video_dir, val_label_dir, device, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b99957f0-7166-4020-8bdf-802159b50e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40b297ab-a3da-4651-8e78-6c56d8e498b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTHuBERTTransformer(nn.Module):\n",
    "    def __init__(self, vit_base_model,\n",
    "                 hubert_base_model,\n",
    "                 num_classes,\n",
    "                 nhead,\n",
    "                 num_layers,\n",
    "                 seq_len,\n",
    "                small_dataset = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.vit = timm.create_model(vit_base_model, pretrained=True)\n",
    "\n",
    "        #self.processor = Wav2Vec2Processor.from_pretrained(hubert_base_model)\n",
    "        self.hubert = HubertModel.from_pretrained(hubert_base_model)\n",
    "\n",
    "        if small_dataset:\n",
    "            for param in self.vit.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "            for param in self.hubert.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model = self.vit.num_features + self.hubert.config.hidden_size,\n",
    "                                                  nhead = nhead,\n",
    "                                                  dim_feedforward = (self.vit.num_features + self.hubert.config.hidden_size)//2,\n",
    "                                                  batch_first = True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers = num_layers)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(self.vit.num_features + self.hubert.config.hidden_size, num_classes)\n",
    "    def forward(self, video_feature_raw, audio_feature_raw):\n",
    "\n",
    "        audio_feature = self.hubert(audio_feature_raw).last_hidden_state\n",
    "        vit_feature = self.vit.forward_features(video_feature_raw)\n",
    "\n",
    "        combined_features = self.seq_feature_generation(vit_feature, audio_feature, self.seq_len)\n",
    "        #[batch, seq_len, combined_feature_size]\n",
    "\n",
    "        transformer_output = self.transformer_encoder(combined_features)\n",
    "\n",
    "        logits = self.classifier(transformer_output.squeeze(1))\n",
    "        return logits\n",
    "\n",
    "    def seq_feature_generation(self, video_feature, audio_feature, seq_len, pooling = \"mean\"):\n",
    "        #video_feature : (771, 1, 197, 768)\n",
    "        #audio_feature : [1, 773, 1024]\n",
    "        video_feature = torch.tensor(video_feature, dtype=torch.float32)\n",
    "        audio_feature = torch.tensor(audio_feature, dtype=torch.float32)\n",
    "    \n",
    "        video_feature = video_feature.permute(1,0,2,3)\n",
    "        \n",
    "        if pooling == \"mean\":\n",
    "            video_feature = torch.mean(video_feature, dim = 2, keepdim=False)\n",
    "        elif pooling == \"max\":\n",
    "            video_feature = torch.max(video_feature, dim = 2, keepdim=False)[0]\n",
    "    \n",
    "        max_seq = min(video_feature.shape[1], audio_feature.shape[1])\n",
    "        video_feature = video_feature[:, :max_seq, :]\n",
    "        audio_feature = audio_feature[:, :max_seq, :]\n",
    "        combined_feature = torch.cat([video_feature, audio_feature], dim = -1)\n",
    "        #[1, max_seq, 1024 + 768]\n",
    "        \n",
    "        if max_seq < seq_len:\n",
    "            # Pad both features to seq_len along the sequence dimension\n",
    "            combined_sequences = F.pad(combined_feature, (0, 0, 0, seq_len - max_seq))\n",
    "        else:\n",
    "            num_complete_seqs = max_seq // seq_len\n",
    "            combined_sequences = combined_feature[:,:num_complete_seqs*seq_len, :].view(-1, seq_len, combined_feature.shape[-1])\n",
    "        #[batch, seq_len, combined_feature_size]\n",
    "        return combined_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b090782-0e59-40b8-9d00-37273d74e45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HubertModel were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ViTHuBERTTransformer(\n",
    "    vit_base_model = 'vit_base_patch16_224',\n",
    "    hubert_base_model = \"facebook/hubert-large-ls960-ft\",\n",
    "    num_classes = 27,\n",
    "    nhead = 8,\n",
    "    num_layers = 6,\n",
    "    seq_len = 10,\n",
    "    small_dataset = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c78ddc9b-266c-4fbc-924b-140e988f3411",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.96)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b21e97c-a74f-4a5e-9506-752e0d45d5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbc0ba83-01eb-4608-abd9-e68997ed18fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                              | 0/29 [00:10<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in output.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "chunk:   0%|                                                                           | 0/2275 [00:00<?, ?it/s, now=None]\u001b[A\n",
      "chunk:  20%|████████████▍                                                  | 451/2275 [00:00<00:00, 4509.34it/s, now=None]\u001b[A\n",
      "chunk:  41%|█████████████████████████▊                                     | 930/2275 [00:00<00:00, 4627.54it/s, now=None]\u001b[A\n",
      "chunk:  61%|██████████████████████████████████████                        | 1395/2275 [00:00<00:00, 4611.03it/s, now=None]\u001b[A\n",
      "chunk:  82%|██████████████████████████████████████████████████▋           | 1860/2275 [00:00<00:00, 4613.53it/s, now=None]\u001b[A\n",
      "  0%|                                                                                              | 0/29 [00:11<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "torch.Size([5153, 3, 224, 224])\n",
      "torch.Size([1, 1650624])\n",
      "torch.Size([5153, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                              | 0/29 [00:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.59 GiB. GPU 0 has a total capacity of 11.74 GiB of which 541.62 MiB is free. Including non-PyTorch memory, this process has 10.71 GiB memory in use. Of the allocated memory 10.31 GiB is allocated by PyTorch, and 105.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, dataloader_train, dataloader_valid, loss_fn, max_iter, scheduler, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(audio_feature\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 35\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_feature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, labels)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     37\u001b[0m loss_cumulative \u001b[38;5;241m=\u001b[39m loss_cumulative \u001b[38;5;241m+\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 35\u001b[0m, in \u001b[0;36mViTHuBERTTransformer.forward\u001b[0;34m(self, video_feature_raw, audio_feature_raw)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_feature_raw, audio_feature_raw):\n\u001b[0;32m---> 35\u001b[0m     audio_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhubert\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_feature_raw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     36\u001b[0m     vit_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvit\u001b[38;5;241m.\u001b[39mforward_features(video_feature_raw)\n\u001b[1;32m     38\u001b[0m     combined_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_feature_generation(vit_feature, audio_feature, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len)\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.8/site-packages/transformers/models/hubert/modeling_hubert.py:1088\u001b[0m, in \u001b[0;36mHubertModel.forward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1085\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_projection(extract_features)\n\u001b[1;32m   1086\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_hidden_states(hidden_states, mask_time_indices\u001b[38;5;241m=\u001b[39mmask_time_indices)\n\u001b[0;32m-> 1088\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1096\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.8/site-packages/transformers/models/hubert/modeling_hubert.py:819\u001b[0m, in \u001b[0;36mHubertEncoderStableLayerNorm.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    812\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    813\u001b[0m             layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    814\u001b[0m             hidden_states,\n\u001b[1;32m    815\u001b[0m             attention_mask,\n\u001b[1;32m    816\u001b[0m             output_attentions,\n\u001b[1;32m    817\u001b[0m         )\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 819\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    822\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_the_layer:\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.8/site-packages/transformers/models/hubert/modeling_hubert.py:658\u001b[0m, in \u001b[0;36mHubertEncoderLayerStableLayerNorm.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    656\u001b[0m attn_residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    657\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 658\u001b[0m hidden_states, attn_weights, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    661\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    662\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m attn_residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.8/site-packages/transformers/models/hubert/modeling_hubert.py:501\u001b[0m, in \u001b[0;36mHubertAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    498\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mview(bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len) \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[1;32m    499\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len)\n\u001b[0;32m--> 501\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer_head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer_head_mask\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,):\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.8/site-packages/torch/nn/functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1856\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1860\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.59 GiB. GPU 0 has a total capacity of 11.74 GiB of which 541.62 MiB is free. Including non-PyTorch memory, this process has 10.71 GiB memory in use. Of the allocated memory 10.31 GiB is allocated by PyTorch, and 105.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train(model, opt, train_loader, val_loader, loss_function,max_iter=1, scheduler=scheduler, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ce801-81e1-4f35-af04-b1b7c90947ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4692e2e-8c77-4f2d-8dfd-9cc09b4f566f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (multimodal)",
   "language": "python",
   "name": "multimodal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
