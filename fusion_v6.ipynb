{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52de4466-fbca-4d7b-8d7c-e863a2549e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/bipengwa/.conda/envs/llama/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Processor, HubertModel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "154aa4bc-bad9-4e03-841c-aa98d515cbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 25 23:34:20 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     Off | 00000000:81:00.0 Off |                    0 |\n",
      "|  0%   47C    P0              74W / 300W |      7MiB / 46068MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a0609f-be71-4ff5-b522-b1ac08bcc585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "163c9bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4172cc53-a27f-413d-8e41-401c320544c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls /project/msoleyma_1026/Aff-Wild2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bf3387b-ea8b-465d-a205-c282c09f8cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_feature = torch.load(\"/project/msoleyma_1026/Aff-Wild2/audio_feature/103-30-384x480.pt\", map_location=torch.device(device)).to(device)\n",
    "# video_feature = torch.tensor(np.load(\"/project/msoleyma_1026/Aff-Wild2/video_feature/103-30-384x480.npy\")).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7dae0ea-9c3a-4da3-a6cf-1e2286ebdb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_feature_generation(video_feature, audio_feature, seq_len, pooling = \"mean\"):\n",
    "    #video_feature : (771, 1, 197, 768)\n",
    "    #audio_feature : [1, 773, 1024]\n",
    "    video_feature = torch.tensor(video_feature, dtype=torch.float32).to(device)\n",
    "    audio_feature = audio_feature.to(dtype=torch.float32)\n",
    "\n",
    "    video_feature = video_feature.permute(1,0,2,3)\n",
    "    \n",
    "    if pooling == \"mean\":\n",
    "        video_feature = torch.mean(video_feature, dim = 2, keepdim=False)\n",
    "    elif pooling == \"max\":\n",
    "        video_feature = torch.max(video_feature, dim = 2, keepdim=False)[0]\n",
    "\n",
    "    max_seq = min(video_feature.shape[1], audio_feature.shape[1])\n",
    "    video_feature = video_feature[:, :max_seq, :]\n",
    "    audio_feature = audio_feature[:, :max_seq, :]\n",
    "    combined_feature = torch.cat([video_feature, audio_feature], dim = -1)\n",
    "    #[1, max_seq, 1024 + 768]\n",
    "    \n",
    "    if max_seq < seq_len:\n",
    "        # Pad both features to seq_len along the sequence dimension\n",
    "        combined_sequences = F.pad(combined_feature, (0, 0, 0, seq_len - max_seq))\n",
    "    else:\n",
    "        num_complete_seqs = max_seq // seq_len\n",
    "        combined_sequences = combined_feature[:,:num_complete_seqs*seq_len, :].view(-1, seq_len, combined_feature.shape[-1])\n",
    "    #[-1, seq_len, combined_feature_size]\n",
    "    return combined_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0836a93d-b837-442e-92ac-f076d35cab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence = seq_feature_generation(video_feature, audio_feature, seq_len = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11a603ec-8b67-424a-a0a2-a1d3ec207c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a9c31ab-cc7b-47bf-b193-242ce184ac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTHuBERTTransformer(nn.Module):\n",
    "    def __init__(self, vit_base_model,\n",
    "                 hubert_base_model,\n",
    "                 num_classes,\n",
    "                 nhead,\n",
    "                 num_layers,\n",
    "                small_dataset = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vit = timm.create_model(vit_base_model, pretrained=True)\n",
    "\n",
    "        #self.processor = Wav2Vec2Processor.from_pretrained(hubert_base_model)\n",
    "        self.hubert = HubertModel.from_pretrained(hubert_base_model)\n",
    "\n",
    "        if small_dataset:\n",
    "            for param in self.vit.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "            for param in self.hubert.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model = self.vit.num_features + self.hubert.config.hidden_size,\n",
    "                                                  nhead = nhead,\n",
    "                                                  dim_feedforward = (self.vit.num_features + self.hubert.config.hidden_size)//2,\n",
    "                                                  batch_first = True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers = num_layers)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(self.vit.num_features + self.hubert.config.hidden_size, num_classes)\n",
    "    def forward(self, video_feature_raw, audio_feature_raw):\n",
    "\n",
    "        vit_feature = self.vit.forward_features(video_feature_raw)\n",
    "        audio_feature = self.hubert(audio_feature_raw).last_hidden_state\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = torch.cat((vit_feature, audio_feature), dim=1)\n",
    "\n",
    "        transformer_output = self.transformer_encoder(combined_features)\n",
    "\n",
    "        logits = self.classifier(transformer_output.squeeze(1))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bab43e8b-6d44-4d2d-ac47-12ac8142d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTHuBERTTransformer_prepossed(nn.Module):\n",
    "    def __init__(self, vit_base_model,\n",
    "                 hubert_base_model,\n",
    "                 num_classes,\n",
    "                 nhead,\n",
    "                 num_layers,\n",
    "                small_dataset = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vit = timm.create_model(vit_base_model, pretrained=True)\n",
    "\n",
    "        #self.processor = Wav2Vec2Processor.from_pretrained(hubert_base_model)\n",
    "        self.hubert = HubertModel.from_pretrained(hubert_base_model)\n",
    "\n",
    "        if small_dataset:\n",
    "            for param in self.vit.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "            for param in self.hubert.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "        print(self.vit.num_features + self.hubert.config.hidden_size)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model = self.vit.num_features + self.hubert.config.hidden_size,\n",
    "                                                  nhead = nhead,\n",
    "                                                  dim_feedforward = (self.vit.num_features + self.hubert.config.hidden_size)//2,\n",
    "                                                  batch_first = True)\n",
    "        # print(encoder_layer.head_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers = num_layers)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(self.vit.num_features + self.hubert.config.hidden_size, num_classes)\n",
    "        self._init_transformer_weights()\n",
    "\n",
    "    def _init_transformer_weights(self):\n",
    "        # Initialize only the transformer encoder layers\n",
    "        for layer in self.transformer_encoder.layers:\n",
    "            for module in layer.modules():\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(module.weight)\n",
    "                    if module.bias is not None:\n",
    "                        nn.init.constant_(module.bias, 0)\n",
    "                # Optionally initialize other components like multi-head attention within the transformer layer\n",
    "                # Depending on the PyTorch implementation, you may need to access sub-components directly\n",
    "\n",
    "    def forward(self, combined_feature):\n",
    "\n",
    "        transformer_output = self.transformer_encoder(combined_feature)\n",
    "\n",
    "        logits = self.classifier(transformer_output.squeeze(1))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77f35470-a21f-4420-a0f2-e633d04cda1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1792\n"
     ]
    }
   ],
   "source": [
    "model = ViTHuBERTTransformer_prepossed(\n",
    "    vit_base_model = 'vit_base_patch16_224',\n",
    "    hubert_base_model = \"facebook/hubert-large-ls960-ft\",\n",
    "    num_classes = 12,\n",
    "    nhead = 16,\n",
    "    num_layers = 18,\n",
    "    small_dataset = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afadb220-4a2d-4229-b466-15848b37abdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTHuBERTTransformer_prepossed(\n",
       "  (vit): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       "  (hubert): HubertModel(\n",
       "    (feature_extractor): HubertFeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): HubertLayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1): HubertLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (2): HubertLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (3): HubertLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (4): HubertLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5): HubertLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (6): HubertLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): HubertFeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): HubertEncoderStableLayerNorm(\n",
       "      (pos_conv_embed): HubertPositionalConvEmbedding(\n",
       "        (conv): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "        (padding): HubertSamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (16): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (17): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (18): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (19): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (20): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (21): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (22): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (23): HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=896, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=896, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=896, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=896, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=896, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=896, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=896, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=896, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=896, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=896, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=896, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=896, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=896, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=896, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=896, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=896, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=896, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=896, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=896, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=896, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=896, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=896, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=896, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=896, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (12): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=896, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=896, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (13): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=896, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=896, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (14): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=896, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=896, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (15): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=896, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=896, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (16): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=896, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=896, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (17): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=896, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=896, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=1792, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91296559-d14a-4620-8158-4689e3b0c602",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for param in model.parameters():\n",
    "#     print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4aa5a74b-7bad-43da-8ae0-b5bea2b9f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    loss_cumulative = 0.\n",
    "    eval_labels = []\n",
    "    eval_preds = []\n",
    "    sub_batch_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluate\"):\n",
    "            for inputs, labels in batch:\n",
    "                inputs, labels = inputs.squeeze().to(device), labels.squeeze().to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss_cumulative += loss.item()\n",
    "                # print(loss_cumulative)\n",
    "                preds = torch.sigmoid(outputs) if not hasattr(loss_fn, 'activation') else outputs\n",
    "                preds = (preds > 0.5).int()\n",
    "                eval_labels.append(labels.cpu().numpy().reshape(-1,12))\n",
    "                eval_preds.append(preds.cpu().numpy().reshape(-1,12))\n",
    "                sub_batch_count+=1\n",
    "    eval_labels = np.concatenate(eval_labels, axis=0)\n",
    "    eval_preds = np.concatenate(eval_preds, axis=0)\n",
    "    # print(eval_labels.shape, eval_preds.shape)\n",
    "    eval_f1_score = f1_score(eval_labels, eval_preds, average='macro')\n",
    "    eval_recall_score = recall_score(eval_labels, eval_preds, average='macro')\n",
    "    eval_precision_score = precision_score(eval_labels, eval_preds, average='macro')\n",
    "\n",
    "    return loss_cumulative / sub_batch_count, eval_f1_score, eval_recall_score, eval_precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "342cf6ca-4098-449d-9480-92cba6e8c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglinspace(rate, step, end=None):\n",
    "    t = 0\n",
    "    while end is None or t <= end:\n",
    "        yield t\n",
    "        t = int(t + 1 + step * (1 - math.exp(-t * rate / step)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a223d80-73fa-49cd-b61d-a165c9613063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader_train, dataloader_valid, loss_fn,\n",
    "             max_iter=101, scheduler=None, device=\"cpu\"):\n",
    "    model.to(device = device, dtype=torch.float32)\n",
    "    print(device)\n",
    "    checkpoint_generator = loglinspace(0.3, 5)\n",
    "    checkpoint = next(checkpoint_generator)\n",
    "    start_time = time.time()\n",
    "    run_name = \"vithubertformer\"\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(run_name + '.torch')['state'])\n",
    "    except:\n",
    "        results = {}\n",
    "        history = []\n",
    "        s0 = 0\n",
    "    else:\n",
    "        print('read torch history')\n",
    "        results = torch.load(run_name + '.torch')\n",
    "        history = results['history']\n",
    "        s0 = history[-1]['step'] + 1\n",
    "        \n",
    "    \n",
    "    for step in range(max_iter):\n",
    "        if step == 0:\n",
    "            for i in range(1):\n",
    "                valid_avg_loss, val_f1_score, val_recall_score, val_precision_score= evaluate(model, dataloader_valid, loss_fn, device)\n",
    "                print(f'{i}',f'valid_avg_loss before training: {valid_avg_loss:8.4f}', f'val_f1_score before training: , {val_f1_score:8.4f}')\n",
    "                print(f'{i}',f'val_recall_score before training: {val_recall_score:8.4f}', f'val_precision_score before training: , {val_precision_score:8.4f}')\n",
    "\n",
    "        \n",
    "        model.train()\n",
    "        loss_cumulative = 0.\n",
    "\n",
    "        for batch in tqdm(dataloader_train, desc=\"Training\"):\n",
    "            for inputs, labels in batch:\n",
    "                inputs, labels = inputs.squeeze().to(device), labels.squeeze().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_cumulative += loss.item()\n",
    "    \n",
    "            wall = time.time() - start_time\n",
    "        if step == checkpoint:\n",
    "            checkpoint = next(checkpoint_generator)\n",
    "            assert checkpoint > step\n",
    "\n",
    "            valid_avg_loss, val_f1_score, val_recall_score, val_precision_score = evaluate(model, dataloader_valid, loss_fn, device)\n",
    "\n",
    "            history.append({\n",
    "                'step': s0 + step,\n",
    "                'wall': wall,\n",
    "                'batch': {\n",
    "                    'loss': loss.item(),\n",
    "                },\n",
    "                'valid': {\n",
    "                    'loss': valid_avg_loss,\n",
    "                },\n",
    "            })\n",
    "\n",
    "            results = {\n",
    "                'history': history,\n",
    "                'state': model.state_dict()\n",
    "            }\n",
    "\n",
    "            print(f\"epoch {step + 1:4d}   \" +\n",
    "                  f\"valid loss mse= {valid_avg_loss:8.4f}   \" +\n",
    "                  f\"wall = {time.strftime('%H:%M:%S', time.gmtime(wall))}  \" +\n",
    "                  f\"val_f1_score = {val_f1_score:8.4f}  \\n\" +\n",
    "                  f\"val_recall_score = {val_recall_score:8.4f}  \" +\n",
    "                  f\"val_precision_score = {val_precision_score:8.4f}  \"\n",
    "                 )\n",
    "            print('Training loss: ',loss_cumulative)\n",
    "\n",
    "            with open(run_name + '.torch', 'wb') as f:\n",
    "                torch.save(results, f)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            print_learning_rates(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feb5a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioVideoDataset(Dataset):\n",
    "    def __init__(self, video_dir, audio_dir, label_dir):\n",
    "        self.video_dir = video_dir\n",
    "        self.audio_dir = audio_dir\n",
    "        self.label_dir = label_dir\n",
    "\n",
    "        # Collect all label files, and construct corresponding video and audio file paths\n",
    "        self.entries = []\n",
    "        for label_file in sorted(os.listdir(label_dir)):\n",
    "            if label_file.endswith('.txt'):\n",
    "                base_name = os.path.splitext(label_file)[0]\n",
    "                video_file = os.path.join(video_dir, f\"{base_name}.npy\")\n",
    "                audio_file = os.path.join(audio_dir, f\"{base_name}.pt\")\n",
    "                label_file_path = os.path.join(label_dir, label_file)\n",
    "                \n",
    "                # Add entry only if corresponding video and audio files exist\n",
    "                if os.path.exists(video_file) and os.path.exists(audio_file):\n",
    "                    self.entries.append((video_file, audio_file, label_file_path))\n",
    "                else:\n",
    "                    print(f\"Missing video or audio file for {label_file}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_file, audio_file, label_file = self.entries[idx]\n",
    "        video_feature = np.load(video_file)\n",
    "        audio_feature = torch.load(audio_file)\n",
    "        labels = np.loadtxt(label_file, skiprows=1, delimiter=',')\n",
    "\n",
    "\n",
    "        seq_len = 10  # Define the desired sequence length\n",
    "        min_len = min(len(labels), video_feature.shape[0], audio_feature.shape[1])\n",
    "\n",
    "        # Further truncate data to the minimum length across modalities\n",
    "        labels = labels[:min_len, :]\n",
    "        video_feature = video_feature[:min_len, :, :, :]\n",
    "        audio_feature = audio_feature[:, :min_len, :]\n",
    "\n",
    "        # Find indices where all labels are binary\n",
    "        binary_indices = np.all(np.isin(labels, [0, 1]), axis=1)\n",
    "        # Filter out non-binary frames\n",
    "        labels = labels[binary_indices, :]\n",
    "        video_feature = video_feature[binary_indices, :, :, :]\n",
    "        audio_feature = audio_feature[:, binary_indices, :]  # Adjust this if necessary\n",
    "        \n",
    "        combined_features = seq_feature_generation(video_feature, audio_feature, seq_len)  # Adjust device as needed\n",
    "\n",
    "        label_sequences = labels[:combined_features.shape[0] * seq_len].reshape(-1, seq_len, 12)\n",
    "\n",
    "        return combined_features, label_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f3669f4-04ce-4420-b58f-4befb0f136dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioVideoDataset2(Dataset):\n",
    "    def __init__(self, video_dir, audio_dir, label_dir, device='cpu'):\n",
    "        print(device)\n",
    "        self.device = device\n",
    "        self.entries = []\n",
    "        \n",
    "        # Load all data into memory\n",
    "        for label_file in sorted(os.listdir(label_dir)):\n",
    "            if label_file.endswith('.txt'):\n",
    "                base_name = os.path.splitext(label_file)[0]\n",
    "                video_file = os.path.join(video_dir, f\"{base_name}.pt\")\n",
    "                audio_file = os.path.join(audio_dir, f\"{base_name}.pt\")\n",
    "                label_file_path = os.path.join(label_dir, label_file)\n",
    "                \n",
    "                if os.path.exists(video_file) and os.path.exists(audio_file):\n",
    "                    # Load features and labels\n",
    "                    video_feature = torch.load(video_file).permute(1,0,2)\n",
    "                    audio_feature = torch.load(audio_file)\n",
    "                    labels = np.loadtxt(label_file_path, skiprows=1, delimiter=',')\n",
    "\n",
    "                    min_len = min(len(labels), video_feature.shape[1], audio_feature.shape[1])\n",
    "                    labels = labels[:min_len, :]\n",
    "                    # print(video_feature.shape, audio_feature.shape)\n",
    "                    video_feature = video_feature[:, :min_len, :]\n",
    "                    audio_feature = audio_feature[:, :min_len, :]\n",
    "\n",
    "                    binary_indices = np.all(np.isin(labels, [0, 1]), axis=1)\n",
    "                    # Filter out non-binary frames\n",
    "                    labels = labels[binary_indices, :]\n",
    "                    video_feature = video_feature[:, binary_indices, :]\n",
    "                    audio_feature = audio_feature[:, binary_indices, :]\n",
    "                    \n",
    "                    # Move data to the specified device\n",
    "                    video_feature = video_feature.to(self.device)\n",
    "                    audio_feature = audio_feature.to(self.device)\n",
    "                    labels = torch.tensor(labels, dtype=torch.float32).to(self.device)\n",
    "\n",
    "                    self.entries.append((video_feature, audio_feature, labels))\n",
    "                else:\n",
    "                    print(f\"Missing video or audio file for {base_name}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_feature, audio_feature, labels = self.entries[idx]\n",
    "\n",
    "        seq_len = 10  # Desired sequence length for analysis\n",
    "        sub_batch_size = 128  # Desired number of sequences per sub-batch\n",
    "\n",
    "        combined_feature = torch.cat([video_feature, audio_feature], dim=-1)\n",
    "        total_seqs = labels.shape[0] // seq_len * seq_len  # Discard incomplete final sequence\n",
    "\n",
    "        # Collect full sub-batches only\n",
    "        smaller_batches = []\n",
    "        for start_idx in range(0, total_seqs, sub_batch_size * seq_len):\n",
    "            end_idx = start_idx + sub_batch_size * seq_len\n",
    "            if end_idx > total_seqs:\n",
    "                break  # Discard last smaller batch if it doesn't fill the complete sub_batch_size\n",
    "            batch_features = combined_feature[:, start_idx:end_idx, :].view(-1, seq_len, combined_feature.shape[-1])\n",
    "            batch_labels = labels[start_idx:end_idx, :].view(-1, seq_len, 12)\n",
    "            smaller_batches.append((batch_features, batch_labels))\n",
    "\n",
    "        return smaller_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72cd64e8-0c0c-4263-bec0-7d3ddd0307a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioVideoDataset2(Dataset):\n",
    "    def __init__(self, video_dir, audio_dir, label_dir, device='cpu'):\n",
    "        print(device)\n",
    "        self.device = device\n",
    "        self.entries = []\n",
    "        \n",
    "        # Load all data into memory\n",
    "        for label_file in sorted(os.listdir(label_dir)):\n",
    "            if label_file.endswith('.txt'):\n",
    "                base_name = os.path.splitext(label_file)[0]\n",
    "                video_file = os.path.join(video_dir, f\"{base_name}.pt\")\n",
    "                audio_file = os.path.join(audio_dir, f\"{base_name}.pt\")\n",
    "                label_file_path = os.path.join(label_dir, label_file)\n",
    "                \n",
    "                if os.path.exists(video_file) and os.path.exists(audio_file):\n",
    "                    # Load features and labels\n",
    "                    video_feature = torch.load(video_file).permute(1,0,2)\n",
    "                    audio_feature = torch.load(audio_file)\n",
    "                    labels = np.loadtxt(label_file_path, skiprows=1, delimiter=',')\n",
    "\n",
    "                    min_len = min(len(labels), video_feature.shape[1], audio_feature.shape[1])\n",
    "                    labels = labels[:min_len, :]\n",
    "                    # print(video_feature.shape, audio_feature.shape)\n",
    "                    video_feature = video_feature[:, :min_len, :]\n",
    "                    audio_feature = audio_feature[:, :min_len, :]\n",
    "\n",
    "                    binary_indices = np.all(np.isin(labels, [0, 1]), axis=1)\n",
    "                    # Filter out non-binary frames\n",
    "                    labels = labels[binary_indices, :]\n",
    "                    video_feature = video_feature[:, binary_indices, :]\n",
    "                    audio_feature = audio_feature[:, binary_indices, :]\n",
    "                    \n",
    "                    # Move data to the specified device\n",
    "                    video_feature = video_feature.to(self.device)\n",
    "                    audio_feature = audio_feature.to(self.device)\n",
    "                    labels = torch.tensor(labels, dtype=torch.float32).to(self.device)\n",
    "\n",
    "                    self.entries.append((video_feature, audio_feature, labels))\n",
    "                else:\n",
    "                    print(f\"Missing video or audio file for {base_name}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_feature, audio_feature, labels = self.entries[idx]\n",
    "\n",
    "        seq_len = 10  # Desired sequence length for analysis\n",
    "        sub_batch_size = 128  # Desired number of sequences per sub-batch\n",
    "\n",
    "        combined_feature = torch.cat([video_feature, audio_feature], dim=-1)\n",
    "        total_seqs = labels.shape[0] // seq_len * seq_len  # Discard incomplete final sequence\n",
    "\n",
    "        # Collect full sub-batches only\n",
    "        smaller_batches = []\n",
    "        for start_idx in range(0, total_seqs, sub_batch_size * seq_len):\n",
    "            end_idx = start_idx + sub_batch_size * seq_len\n",
    "            if end_idx > total_seqs:\n",
    "                break  # Discard last smaller batch if it doesn't fill the complete sub_batch_size\n",
    "            batch_features = combined_feature[:, start_idx:end_idx, :].view(-1, seq_len, combined_feature.shape[-1])\n",
    "            batch_labels = labels[start_idx:end_idx, :].view(-1, seq_len, 12)\n",
    "            smaller_batches.append((batch_features, batch_labels))\n",
    "\n",
    "        return smaller_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a557f02b-aa25-4f34-a0f5-b3d58764fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioVideoDataset3(Dataset):\n",
    "    def __init__(self, video_dir, audio_dir, label_dir, device):\n",
    "        self.video_dir = video_dir\n",
    "        self.audio_dir = audio_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.entries = []\n",
    "\n",
    "        # Collect all the filenames without loading data into memory\n",
    "        for label_file in sorted(os.listdir(label_dir)):\n",
    "            if label_file.endswith('.txt'):\n",
    "                base_name = os.path.splitext(label_file)[0]\n",
    "                video_file = os.path.join(video_dir, f\"{base_name}.pt\")\n",
    "                audio_file = os.path.join(audio_dir, f\"{base_name}.pt\")\n",
    "                label_file_path = os.path.join(label_dir, label_file)\n",
    "                \n",
    "                if os.path.exists(video_file) and os.path.exists(audio_file):\n",
    "                    self.entries.append((video_file, audio_file, label_file_path))\n",
    "                else:\n",
    "                    print(f\"Missing video or audio file for {base_name}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_file, audio_file, label_file = self.entries[idx]\n",
    "\n",
    "        # Load features and labels\n",
    "        video_feature = torch.load(video_file).permute(1, 0, 2)\n",
    "        audio_feature = torch.load(audio_file)\n",
    "        labels = np.loadtxt(label_file, skiprows=1, delimiter=',')\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        # Ensure consistent lengths across modalities\n",
    "        min_len = min(len(labels), video_feature.shape[1], audio_feature.shape[1])\n",
    "        labels = labels[:min_len, :]\n",
    "        video_feature = video_feature[:, :min_len, :]\n",
    "        audio_feature = audio_feature[:, :min_len, :]\n",
    "\n",
    "        # Filter non-binary frames\n",
    "        binary_indices = np.all(np.isin(labels, [0, 1]), axis=1)\n",
    "        labels = labels[binary_indices, :]\n",
    "        video_feature = video_feature[:, binary_indices, :]\n",
    "        audio_feature = audio_feature[:, binary_indices, :]\n",
    "\n",
    "        # Define the desired sequence length and sub-batch size\n",
    "        seq_len = 10\n",
    "        sub_batch_size = 128\n",
    "\n",
    "        # Concatenate and prepare batches\n",
    "        combined_feature = torch.cat([video_feature.to(device), audio_feature.to(device)], dim=-1)\n",
    "        total_seqs = len(labels) // seq_len * seq_len\n",
    "\n",
    "        smaller_batches = []\n",
    "        for start_idx in range(0, total_seqs, sub_batch_size * seq_len):\n",
    "            end_idx = start_idx + sub_batch_size * seq_len\n",
    "            if end_idx > total_seqs:\n",
    "                break  # Discard last smaller batch if incomplete\n",
    "            # print(start_idx, end_idx, labels.shape)\n",
    "            batch_features = combined_feature[:, start_idx:end_idx, :].view(-1, seq_len, combined_feature.shape[-1])\n",
    "            batch_labels = labels[start_idx:end_idx, :].view(-1, seq_len, 12)\n",
    "            smaller_batches.append((batch_features, batch_labels))\n",
    "\n",
    "        return smaller_batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1b1e1e65-0fd1-4329-89a1-628320cc7dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c6fa2976-734a-44cc-9d20-b9a120849251",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing video or audio file for 10-60-1280x720_right\n",
      "Missing video or audio file for 135-24-1920x1080_left\n",
      "Missing video or audio file for 135-24-1920x1080_right\n",
      "Missing video or audio file for 46-30-484x360_left\n",
      "Missing video or audio file for 46-30-484x360_right\n"
     ]
    }
   ],
   "source": [
    "# train_label_dir = '/project/msoleyma_1026/Aff-Wild2/test/test1/Train_Set/label_30'\n",
    "train_label_dir = '/project/msoleyma_1026/Aff-Wild2/labels/AU_Detection_Challenge/Train_Set'\n",
    "audio_feature_dir = '/project/msoleyma_1026/Aff-Wild2/features4'\n",
    "video_feature_dir = '/project/msoleyma_1026/Aff-Wild2/video_feature_pooled'\n",
    "dataset_train = AudioVideoDataset3(video_feature_dir, audio_feature_dir, train_label_dir, device)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1a76d291-87f9-417b-9423-62461fda8172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "da3ef7bb-599e-4b50-9755-a6fa023a572e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1792"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "768+1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "23d4e5d5-c709-4618-b843-b98089b14813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls /project/msoleyma_1026/Aff-Wild2/test/Validation_Set/label_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "97630e72-95ec-4fcf-bd54-3186bd587582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls /project/msoleyma_1026/Aff-Wild2/test/Validation_Set/video_feature_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bc6a4233-0752-4fc7-a4c7-d8316105f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls /project/msoleyma_1026/Aff-Wild2/test/Validation_Set/audio_feature_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "20a0ba89-bdbf-4a1d-b5b1-a7afddb7fa17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# val_label_dir = '/project/msoleyma_1026/Aff-Wild2/test/test1/Validation_Set/label_10'\n",
    "val_label_dir = '/project/msoleyma_1026/Aff-Wild2/labels/AU_Detection_Challenge/Validation_Set'\n",
    "val_video_feature_dir = \"/project/msoleyma_1026/Aff-Wild2/video_feature_pooled\"\n",
    "val_audio_feature_dir='/project/msoleyma_1026/Aff-Wild2/features4'\n",
    "dataset_val = AudioVideoDataset3(val_video_feature_dir, val_audio_feature_dir, val_label_dir, device)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9edb2208-9644-4b8e-9619-2ea6c1dbf248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_val.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c8875cee-1b36-411f-b2a0-44e920867602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.randint(0, 10, (1287, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f523c768-de83-4b0d-ba66-7093ca3b3274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "39ea1d16-8610-438f-8444-ca9e261d2b84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a[0:1280, :].view(-1, 128, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1b822b7d-3f11-472e-99be-9f74f504dffc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Extract one batch from the dataloader\n",
    "# count=0\n",
    "# for batch in dataloader_val:\n",
    "#     for features, labels in batch:\n",
    "#         print(count, \"Features shape:\", features.shape, \"Labels shape:\", labels.shape)\n",
    "#         # print(\"Labels shape:\", labels.shape)\n",
    "#         # print(\"Features example (one batch):\", features)\n",
    "#         # print(\"Labels example (one batch):\", labels)\n",
    "#         # count+=1\n",
    "#         break  # Break after the first batch to only print one batch of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "32792b2a-174b-4425-b4bb-85e261cd2df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'vithubertformer.torch': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! rm vithubertformer.torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "872b9f3d-85e2-445e-ace1-7519269168ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_learning_rates(optimizer):\n",
    "    for idx, param_group in enumerate(optimizer.param_groups):\n",
    "        print(f\"Learning rate for parameter group {idx}: {param_group['lr']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ce6997-4878-484e-ad26-05f58e6058c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 105/105 [00:18<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 valid_avg_loss before training:   4.6662 val_f1_score before training: ,   0.1551\n",
      "0 val_recall_score before training:   0.3486 val_precision_score before training: ,   0.1841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 55/290 [00:44<03:24,  1.15it/s]"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.96)\n",
    "train(model, opt, dataloader_train, dataloader_val, loss_function,\n",
    "             max_iter=10, scheduler=scheduler, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c114fc1-c87e-4102-9483-86b750585378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf8f5c-8c4d-436e-b277-fa90d6b2d8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_dir = '/project/msoleyma_1026/Aff-Wild2/labels/AU_Detection_Challenge/Train_Set'\n",
    "# audio_feature_dir = '/project/msoleyma_1026/Aff-Wild2/features4'\n",
    "# video_feature_dir = '/project/msoleyma_1026/Aff-Wild2/video_feature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d106e-4245-4b8e-bba4-546693ac4382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_names = [f for f in os.listdir(label_dir) if os.path.isfile(os.path.join(label_dir, f))]\n",
    "# audio_names = [f for f in os.listdir(audio_feature_dir) if os.path.isfile(os.path.join(audio_feature_dir, f))]\n",
    "# video_names = [f for f in os.listdir(video_feature_dir) if os.path.isfile(os.path.join(video_feature_dir, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8cae3-c457-4a6b-a01b-6f4110689f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850ad1dd-53f2-491f-b5ca-3a8b8c490680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# audio_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a45c6-a7be-44d1-8d1a-3354cf55a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# share_names = []\n",
    "# for name in label_names:\n",
    "#     if os.path.basename(name).replace('.txt', '.npy') in video_names:\n",
    "#         share_names.append(name)\n",
    "#     # if os.path.basename(name).replace('.txt', '.pt') in audio_names:\n",
    "#     #     share_names.append(name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da810451-83a2-4b33-9ebd-a2b397096d85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# share_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413783b2-8674-47da-8288-12541d437144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(share_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfde2fd-27b7-4e62-908d-4c5f85956a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(label_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
